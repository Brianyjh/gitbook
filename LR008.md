### Logistic/sigmoid 回归
<img src="https://note.youdao.com/yws/api/personal/file/WEBb507f48ab63fbae3ce4be0fac29d0d73?method=download&shareKey=5ede37b46ecde29fce258f7fae05f908" width=400>

对于$$g\left( z \right) =\frac{1}{1+e^{-z}}$$

令$$z = \theta^{T} x$$

#### 解决分类问题

假定：$$P\left( y=1|x;\theta \right) \,\,=\,\,h_{\theta}\left( x \right) $$

$$P\left( y=0|x;\theta \right) \,\,=\,\,1 -\,\,h_{\theta}\left( x \right) $$

假定概率独立同分布，即可算似然函数

$$l\left( \theta \right) =\log L\left( \theta \right) =\sum_{i=1}^m{y^{\left( i \right)}\log h\left( x^{\left( i \right)} \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-h\left( x^{\left( i \right)} \right) \right)}$$

$$\frac{\partial l\left( \theta \right)}{\partial \theta_j}=\sum\limits_{i=1}^m{\left( y^{\left( i \right)}-g\left( \theta ^Tx^{\left( i \right)} \right) \right) \cdot x_{j}^{\left( i \right)}}$$

- 求偏导原因：用**梯度下降**进行参数优化(这里实际上叫梯度上升，本质和梯度下降一样)

#### Logistic  回归参数的学习规则

$$\theta_j:=\theta_j+\alpha \left( y^{\left( i \right)}-h_{\theta}\left( x^{\left( i \right)} \right) \right) x_{j}^{\left( i \right)}$$

#### 比较上面的结果和线性回归的结论的差别
- 他们竟然有**相同的形式**！
当$$h_\theta(x) = \theta^{T}x$$,那么对应的回归为 Linear Regression

当$$h_\theta(x) =  \frac{1}{1+e^{-\theta ^Tx}}    $$,那么对应的回归为Logistic Regression

<img src="https://note.youdao.com/yws/api/personal/file/WEB5eb0b36143d304c96774a729a1c4cff9?method=download&shareKey=1a1c162ecb801c6e3ff22b1e8f3694e2">


<img src="https://note.youdao.com/yws/api/personal/file/WEB3128124a0dc4747990fa943b5e2ad284?method=download&shareKey=58a6ff223578dd05c9574d42be3a63e5">


