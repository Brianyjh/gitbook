###过拟合现象
<img src = "https://note.youdao.com/yws/api/personal/file/WEBce786099e46971828a8253e630b09a7f?method=download&shareKey=0039b84f45dc2c43515cb12652918530" >

一般对于**参数过大**的目标函数，常常会导致过拟合现象

我们的目标函数为$$J\left( \vec{\theta} \right) =\frac{1}{2}\sum_{i=1}^m{\left( h_{\vec{\theta}}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}$$

我们希望参数不要太大，防止龙格现象(过拟合)，我们可以将目标函数增加平方和损失：

$$J\left( \vec{\theta} \right) =\frac{1}{2}\sum\limits_{i=1}^m{\left( h_{\vec{\theta}}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}+\lambda \sum\limits_{j=1}^n{\theta _{j}^{2}}$$

也就是说，此时，我们希望$$\lambda \sum\limits_{j=1}^n{\theta_{j}^{2}}$$不要过大，这个$$\lambda \sum_{j=1}^n{\theta _{j}^{2}}$$就是**正则项**,以平方的方式进行的做法称为“L2-norm”

这样就会降低过拟合的情况。这样的情况在线性回归里有特定的名字叫Ridge 回归(岭回归)

- LASSO回归 对应“L1-norm” 
- Least Angle Regression

对于 $$J\left( \vec{\theta} \right) =\frac{1}{2}\sum\limits_{i=1}^m{\left( h_{\vec{\theta}}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}+\lambda \sum\limits_{j=1}^n{\theta _{j}^{2}}$$

相当于 $$J\left( \vec{\theta} \right) =\frac{1}{2}\left( x\theta -y \right) ^T\left( x\theta -y \right) \,\,+\,\,\lambda \theta ^T\theta $$

求出来是$$\theta \,\,=\,\,\left( X^TX+\lambda I \right) ^{-1}X^Ty$$





### 正则项 与 防止过拟合
<img src="https://note.youdao.com/yws/api/personal/file/WEB87740dc853a4aaff2c761d009bc4b33f?method=download&shareKey=47e60377750ba899143f5db08d95d5d3">
### 稀疏解
稀疏解就是除去不重要的解(特征选择)

稀疏解求出向量$$\theta $$里面各个元素的和(算出不为0的元素的和)
这样的正则做法，我们称为"L0-norm"，这样的解法是一个NP问题

因此，我们需要做简化。

LASSO 采用的正则做法为"L1-norm"

让$$\theta_i < \varepsilon (i=1,2,...,n)$$

按照拉格朗日乘子法，我们的约束条件为$$\theta_i < \varepsilon (i=1,2,...,n)$$

原来的目标函数为$$\frac{1}{2}\sum_{i=1}^m{\left( h_{\vec{\theta}}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}$$

P.S.这里我们设置约束条件，目的是为了让参数不要太大

所以加了约束条件后，我们的式子为$$\frac{1}{2}\sum\limits_{i=1}^m{\left( h_{\vec{\theta}}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2} +\lambda \sum\limits_{i=1}^n{\left| \theta_i -\varepsilon  \right|}\left( i=1,2,...,n \right)  $$

这里稍作简化，式子可化简为：$$\frac{1}{2}\sum\limits_{i=1}^m{\left( h_{\vec{\theta}}\left( x^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2} +\lambda \sum\limits_{i=1}^n{\left| \theta_i   \right|} $$

-这个式子即为我们的LASSO(解释LASSO)
