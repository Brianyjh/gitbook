### 梯度下降

- 我们用SVD来求解很麻烦(需要迭代很多次)，可不可以绕过这一步呢，当然可以。

- 初始化$$\theta$$(随机初始化)
- 沿着负梯度方向迭代
- 如果$$x$$的维度在100以下，直接套原来公式算很方便，但如果维度在100以上，建议用梯度下降法

<img src="https://note.youdao.com/yws/api/personal/file/WEB9643966df8af79172592de8e14c14d18?method=download&shareKey=8ce1d0885e34e82a08e1c3745bf5d46c">

#####梯度方向

$$\frac{\partial}{\partial \theta _j}J\left( \theta \right) =\,\,\frac{\partial}{\partial \theta _j}\frac{1}{2}\left( h_{\theta}\left( x \right) -y \right) ^2$$

$$=2\cdot \frac{1}{2}\left( h_{\theta}\left( x \right) -y \right) \cdot \frac{\partial}{\partial \theta _j}\left( h_{\theta}\left( x \right) -y \right) $$

$$=\left( h_{\theta}\left( x \right) -y \right) \cdot \frac{\partial}{\partial \theta _j}\left( \sum_{i=0}^n{\theta _ix_i-y} \right) $$

$$=\left( h_{\theta}\left( x \right) -y \right) x_j$$

###批量梯度算法BGD
- 批量计算，虽然算不到最优解，但是堪用
- 批量梯度的本质就是贪心
<img src="https://note.youdao.com/yws/api/personal/file/WEBc7538016350337666c4c7ef8d320137b?method=download&shareKey=50cdf3237209da37af81cb31b171ad82"width=400>

###随机梯度下降SGD

<img src="https://note.youdao.com/yws/api/personal/file/WEBad48576c41975857e9a12ec9945c581c?method=download&shareKey=3cd9a0567229d0b03c0702fb66baa448" width=400>

### BGD与SGD选哪个

- 优先选SGD
- SGD 有时能够跳出*局部最小值*
- 但是SGD 过于激进
- 所以...
### Mini-batch

- 实际上Mini-batch被用的频率高于SGD与BGD
- 如果不是每拿到一个样本即更改梯度，而是若干个样本的平均梯度作为更新方向，则是mini-batch梯度下降算法。


